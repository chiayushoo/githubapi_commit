{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 Set up to call API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lib\n",
    "import requests\n",
    "from requests import get\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup owner name , access_token, and headers \n",
    "access_token='{sealed}' \n",
    "headers = {    \"Authorization\": f\"Token {access_token}\",\n",
    "    \"Accept\": \"application/vnd.github+json\"}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Get Data of L6M commits of airflow repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function to call github API\n",
    "def get_commits_l6m(**kwargs):\n",
    "    \"\"\"\n",
    "    Returns the number of commits to a GitHub repository.\n",
    "    \"\"\"\n",
    "    biz_date = datetime.today() + relativedelta(months=-6)\n",
    "    biz_date = biz_date.isoformat()\n",
    "    params = {\n",
    "        \"since\":biz_date,\n",
    "        \"per_page\": 100,  # Number of results per page\n",
    "        \"page\": 1  # Page number to retrieve\n",
    "    }\n",
    "    #initial request\n",
    "    response = requests.get('https://api.github.com/repos/apache/airflow/commits', params)\n",
    "    results = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        #set a marker to see if we retrive all of the results\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if 'next' in response.links:\n",
    "                params[\"page\"] += 1\n",
    "                response = requests.get('https://api.github.com/repos/apache/airflow/commits', params)\n",
    "                if response.status_code == 200:\n",
    "                    results.extend(response.json())\n",
    "                else:\n",
    "                    # There was an error with the request, so stop processing\n",
    "                    done = True\n",
    "            else: \n",
    "                 # There are no more pages of results, so stop processing\n",
    "                done = True            \n",
    "    return  [i[\"commit\"][\"author\"] for i in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_data = get_commits_l6m()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "print(type(spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe: 1. have a grass_date to partition the table 2. turn datetime of commit from ISO8601 to datetime type (for mysql)\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "columns = [\"datetime\", \"email\", \"name\"]\n",
    "df_commit = spark.createDataFrame(data=commit_data, schema = columns)\n",
    "df_commit = df_commit.withColumn('grass_date', date_format(current_timestamp(), 'yyyy-MM-dd'))\n",
    "df_commit = df_commit.withColumn(\"datetime\",date_format('datetime',\"yyyy-MM-dd HH:mm:ss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----------------+----------+\n",
      "|           datetime|               email|            name|grass_date|\n",
      "+-------------------+--------------------+----------------+----------+\n",
      "|2022-12-17 09:40:15|Andrey.Anshin@tar...|   Andrey Anshin|2022-12-18|\n",
      "|2022-12-17 06:24:33|49878111+TohnJhom...|     John Thomas|2022-12-18|\n",
      "|2022-12-17 02:04:36| brent@astronomer.io|   Brent Bovenzi|2022-12-18|\n",
      "|2022-12-16 18:33:56|  uranusjr@gmail.com|  Tzu-ping Chung|2022-12-18|\n",
      "|2022-12-16 17:37:10|    jarek@potiuk.com|    Jarek Potiuk|2022-12-18|\n",
      "|2022-12-16 06:59:07|103602455+syedahs...|   Syed Hussaain|2022-12-18|\n",
      "|2022-12-16 04:29:15|amoghrajesh1999@g...|     Amogh Desai|2022-12-18|\n",
      "|2022-12-16 04:05:10|45845474+eladkal@...|         eladkal|2022-12-18|\n",
      "|2022-12-16 00:43:52|  uranusjr@gmail.com|  Tzu-ping Chung|2022-12-18|\n",
      "|2022-12-16 00:43:17|  uranusjr@gmail.com|  Tzu-ping Chung|2022-12-18|\n",
      "|2022-12-15 18:29:25|cmachalow@linkedi...|Charles Machalow|2022-12-18|\n",
      "|2022-12-15 17:20:25|  uranusjr@gmail.com|  Tzu-ping Chung|2022-12-18|\n",
      "|2022-12-15 15:28:09|splendidzigy24@gm...|Ephraim Anierobi|2022-12-18|\n",
      "|2022-12-15 06:41:44| ferruzzi@amazon.com|     D. Ferruzzi|2022-12-18|\n",
      "|2022-12-15 03:45:45| brent@astronomer.io|   Brent Bovenzi|2022-12-18|\n",
      "|2022-12-14 16:51:23|6147573+doiken@us...|          doiken|2022-12-18|\n",
      "|2022-12-14 16:48:06|mailbowrna@gmail.com|          Bowrna|2022-12-18|\n",
      "|2022-12-14 16:20:27|    jarek@potiuk.com|    Jarek Potiuk|2022-12-18|\n",
      "|2022-12-14 15:51:26|splendidzigy24@gm...|Ephraim Anierobi|2022-12-18|\n",
      "|2022-12-14 15:47:46|107272191+syun64@...|        Sung Yun|2022-12-18|\n",
      "+-------------------+--------------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# the analysis base done\n",
    "df_commit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to my mysql local database\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='kelsy2022',\n",
    "    database='edb'\n",
    ")\n",
    "\n",
    "# Iterate over the rows of the DataFrame and execute an INSERT statement for each row\n",
    "for row in df_commit.rdd.toLocalIterator():\n",
    "    insert_query = f\"INSERT INTO commit_l6m_airflow VALUES ({','.join(['%s'] * len(row))})\"\n",
    "    conn.cursor().execute(insert_query, row)\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tempview table for sparksql\n",
    "df_commit.createOrReplaceTempView(\"commit_l6m\")\n",
    "##after read mysql local table using pyspark, can also call df.createOrReplaceTempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|      min(datetime)|      max(datetime)|\n",
      "+-------------------+-------------------+\n",
      "|2022-06-19 16:53:15|2022-12-17 09:40:15|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is used to check if we cover l6m data\n",
    "sqlDF_0 = spark.sql(\"SELECT min(datetime), max(datetime) FROM commit_l6m\")\n",
    "sqlDF_0.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+\n",
      "|           name|commit_cnt|\n",
      "+---------------+----------+\n",
      "|   Jarek Potiuk|       365|\n",
      "|Daniel Standish|       108|\n",
      "| Tzu-ping Chung|        93|\n",
      "| Jed Cunningham|        77|\n",
      "|  Andrey Anshin|        75|\n",
      "+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#query 1: For the ingested commits, determine the top 5 committers by count of commits and the number  of commits. \n",
    "\n",
    "sqlDF_1 = spark.sql('''\n",
    "SELECT name, commit_cnt \n",
    "FROM \n",
    "  (\n",
    "    SELECT distinct name, email, count(*) as commit_cnt \n",
    "    FROM commit_l6m \n",
    "    WHERE grass_date = current_date\n",
    "    GROUP BY name, email\n",
    "  ) \n",
    "Order By commit_cnt desc \n",
    "LIMIT 5\n",
    "''')\n",
    "sqlDF_1.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|        name|record|\n",
      "+------------+------+\n",
      "|Jarek Potiuk|   139|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#query 2: For the ingested commits, determine the committer with the longest commit streak.  \n",
    "\n",
    "sqlDF_2 = spark.sql('''\n",
    "\n",
    "WITH streak AS (\n",
    "\tSELECT *\n",
    "\tFROM(\n",
    "        SELECT name, email, datetime, lag(name) OVER (PARTITION BY NULL ORDER BY datetime) as prev_committer\n",
    "        FROM commit_l6m\n",
    "\t\tWHERE grass_date = current_date\n",
    "\t\t)\n",
    "\tWHERE name = prev_committer\n",
    ")\n",
    "select name, count(*) as record\n",
    "from streak\n",
    "group by name\n",
    "order by count(name) desc\n",
    "LIMIT 1\n",
    "''')\n",
    "sqlDF_2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+---+--------+\n",
      "|            name|               email|            datetime| dw|hour_itv|\n",
      "+----------------+--------------------+--------------------+---+--------+\n",
      "|  Tzu-ping Chung|  uranusjr@gmail.com|2022-12-16T10:33:56Z|Fri|   18-21|\n",
      "|    Jarek Potiuk|    jarek@potiuk.com|2022-12-16T09:37:10Z|Fri|   15-18|\n",
      "|   Syed Hussaain|103602455+syedahs...|2022-12-15T22:59:07Z|Fri|   06-09|\n",
      "|     Amogh Desai|amoghrajesh1999@g...|2022-12-15T20:29:15Z|Fri|   03-06|\n",
      "|         eladkal|45845474+eladkal@...|2022-12-15T20:05:10Z|Fri|   03-06|\n",
      "|  Tzu-ping Chung|  uranusjr@gmail.com|2022-12-15T16:43:52Z|Fri|   00-03|\n",
      "|  Tzu-ping Chung|  uranusjr@gmail.com|2022-12-15T16:43:17Z|Fri|   00-03|\n",
      "|Charles Machalow|cmachalow@linkedi...|2022-12-15T10:29:25Z|Thu|   18-21|\n",
      "|  Tzu-ping Chung|  uranusjr@gmail.com|2022-12-15T09:20:25Z|Thu|   15-18|\n",
      "|Ephraim Anierobi|splendidzigy24@gm...|2022-12-15T07:28:09Z|Thu|   15-18|\n",
      "|     D. Ferruzzi| ferruzzi@amazon.com|2022-12-14T22:41:44Z|Thu|   06-09|\n",
      "|   Brent Bovenzi| brent@astronomer.io|2022-12-14T19:45:45Z|Thu|   03-06|\n",
      "|          doiken|6147573+doiken@us...|2022-12-14T08:51:23Z|Wed|   15-18|\n",
      "|          Bowrna|mailbowrna@gmail.com|2022-12-14T08:48:06Z|Wed|   15-18|\n",
      "|    Jarek Potiuk|    jarek@potiuk.com|2022-12-14T08:20:27Z|Wed|   15-18|\n",
      "|Ephraim Anierobi|splendidzigy24@gm...|2022-12-14T07:51:26Z|Wed|   15-18|\n",
      "|        Sung Yun|107272191+syun64@...|2022-12-14T07:47:46Z|Wed|   15-18|\n",
      "|         Vincent|97131062+vincbeck...|2022-12-14T07:23:26Z|Wed|   15-18|\n",
      "|  Jed Cunningham|66968678+jedcunni...|2022-12-14T02:18:41Z|Wed|   09-12|\n",
      "|    Jarek Potiuk|    jarek@potiuk.com|2022-12-13T23:55:05Z|Wed|   06-09|\n",
      "+----------------+--------------------+--------------------+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#query 3: For the ingested commits, generate a heatmap of number of commits count by all users by day  of the week and by 3 hour blocks.  \n",
    "\n",
    "##new date_format doc: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
    "##one thing needs to discuss/align: the original time is recorded in UTC time, here I use SG Local Time to do the heatmap\n",
    "sqlDF_3_1 = spark.sql('''\n",
    "SELECT name, email, datetime, date_format(datetime, 'E') as dw,\n",
    "\tcase\n",
    "\t\twhen date_format(datetime, 'H') in ('0', '1', '2') then '00-03'\n",
    "\t\twhen date_format(datetime, 'H') in ('3', '4', '5') then '03-06'\n",
    "\t\twhen date_format(datetime, 'H') in ('6', '7', '8') then '06-09'\n",
    "\t\twhen date_format(datetime, 'H') in ('9', '10', '11') then '09-12'\n",
    "\t\twhen date_format(datetime, 'H') in ('12', '13', '14') then '12-15'\n",
    "\t\twhen date_format(datetime, 'H') in ('15', '16', '17') then '15-18'\n",
    "\t\twhen date_format(datetime, 'H') in ('18', '19', '20') then '18-21'\n",
    "\t\telse '21-00'\n",
    "\tend as hour_itv\n",
    "FROM commit_l6m\n",
    "WHERE grass_date = current_date\n",
    "''')\n",
    "sqlDF_3_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "| dw|00-03|03-06|06-09|09-12|12-15|15-18|18-21|21-00|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|Mon|   19|   28|   30|   38|   39|   37|   31|   37|\n",
      "|Sun|   16|   28|   12|    7|    7|   16|   17|   11|\n",
      "|Sat|   50|   50|   32|   18|    8|   15|   11|   19|\n",
      "|Thu|   37|   60|   47|    7|   19|   40|   41|   50|\n",
      "|Wed|   47|   63|   33|   24|   28|   34|   53|   47|\n",
      "|Fri|   57|   55|   37|   23|    7|   30|   25|   65|\n",
      "|Tue|   56|   46|   33|    7|   15|   36|   60|   65|\n",
      "+---+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlDF_3_1 = spark.sql('''\n",
    "SELECT *\n",
    "FROM\n",
    "  (WITH raw AS\n",
    "    (\n",
    "      SELECT name, email, datetime,\n",
    "      date_format(datetime, 'E') AS dw,\n",
    "      CASE\n",
    "        WHEN date_format(datetime, 'H') IN ('0','1','2') THEN '00-03'\n",
    "        WHEN date_format(datetime, 'H') IN ('3','4','5') THEN '03-06'\n",
    "        WHEN date_format(datetime, 'H') IN ('6','7','8') THEN '06-09'\n",
    "        WHEN date_format(datetime, 'H') IN ('9','10','11') THEN '09-12'\n",
    "        WHEN date_format(datetime, 'H') IN ('12','13','14') THEN '12-15'\n",
    "        WHEN date_format(datetime, 'H') IN ('15','16','17') THEN '15-18'\n",
    "        WHEN date_format(datetime, 'H') IN ('18','19','20') THEN '18-21'\n",
    "        ELSE '21-00'\n",
    "      END AS hour_itv\n",
    "      FROM commit_l6m\n",
    "      WHERE grass_date = current_date\n",
    "    )\n",
    "    SELECT dw, hour_itv, count(*) AS total_commits\n",
    "    FROM raw\n",
    "    GROUP BY dw, hour_itv\n",
    "  ) AS data\n",
    "PIVOT(SUM(total_commits) FOR hour_itv IN ('00-03','03-06','06-09','09-12','12-15','15-18','18-21','21-00'))\n",
    "ORDER BY\n",
    "  CASE dw\n",
    "    WHEN 'Mon' THEN 1\n",
    "    WHEN 'Tuesday' THEN 2\n",
    "    WHEN 'Wednesday' THEN 3\n",
    "    WHEN 'Thursday' THEN 4\n",
    "    WHEN 'Friday' THEN 5\n",
    "    WHEN 'Saturday' THEN 6\n",
    "    ELSE 7\n",
    "  END\n",
    "''')\n",
    "sqlDF_3_1.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 Query from mysql table\n",
    "The above workflow is based on df_commit, which I fetch directly from API. I did insert the table into table commit_l6m_airflow in local mysql database.\n",
    "Hence, the above query can change the source to mysql table.\n",
    "Folllowings are the way to get data from mysql table: df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+----------------+----------+\n",
      "|           datetime|               email|            name|grass_date|\n",
      "+-------------------+--------------------+----------------+----------+\n",
      "|2022-12-17 09:40:15|Andrey.Anshin@tar...|   Andrey Anshin|2022-12-18|\n",
      "|2022-12-17 06:24:33|49878111+TohnJhom...|     John Thomas|2022-12-18|\n",
      "|2022-12-17 02:04:36| brent@astronomer.io|   Brent Bovenzi|2022-12-18|\n",
      "|2022-12-16 18:33:56|  uranusjr@gmail.com|  Tzu-ping Chung|2022-12-18|\n",
      "|2022-12-16 17:37:10|    jarek@potiuk.com|    Jarek Potiuk|2022-12-18|\n",
      "|2022-12-16 06:59:07|103602455+syedahs...|   Syed Hussaain|2022-12-18|\n",
      "|2022-12-16 04:29:15|amoghrajesh1999@g...|     Amogh Desai|2022-12-18|\n",
      "|2022-12-16 04:05:10|45845474+eladkal@...|         eladkal|2022-12-18|\n",
      "|2022-12-16 00:43:52|  uranusjr@gmail.com|  Tzu-ping Chung|2022-12-18|\n",
      "|2022-12-16 00:43:17|  uranusjr@gmail.com|  Tzu-ping Chung|2022-12-18|\n",
      "|2022-12-15 18:29:25|cmachalow@linkedi...|Charles Machalow|2022-12-18|\n",
      "|2022-12-15 17:20:25|  uranusjr@gmail.com|  Tzu-ping Chung|2022-12-18|\n",
      "|2022-12-15 15:28:09|splendidzigy24@gm...|Ephraim Anierobi|2022-12-18|\n",
      "|2022-12-15 06:41:44| ferruzzi@amazon.com|     D. Ferruzzi|2022-12-18|\n",
      "|2022-12-15 03:45:45| brent@astronomer.io|   Brent Bovenzi|2022-12-18|\n",
      "|2022-12-14 16:51:23|6147573+doiken@us...|          doiken|2022-12-18|\n",
      "|2022-12-14 16:48:06|mailbowrna@gmail.com|          Bowrna|2022-12-18|\n",
      "|2022-12-14 16:20:27|    jarek@potiuk.com|    Jarek Potiuk|2022-12-18|\n",
      "|2022-12-14 15:51:26|splendidzigy24@gm...|Ephraim Anierobi|2022-12-18|\n",
      "|2022-12-14 15:47:46|107272191+syun64@...|        Sung Yun|2022-12-18|\n",
      "+-------------------+--------------------+----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a cursor\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='kelsy2022',\n",
    "    database='edb'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "# Select rows from the table\n",
    "query = 'SELECT * FROM commit_l6m_airflow'\n",
    "cursor.execute(query)\n",
    "'''\n",
    "# Get the column names\n",
    "column_names = [column[0] for column in cursor.description]\n",
    "\n",
    "# Store the results in a DataFrame\n",
    "df = pd.DataFrame(cursor.fetchall(), columns=column_names)\n",
    "'''\n",
    "\n",
    "# Get the column names\n",
    "column_names = [column[0] for column in cursor.description]\n",
    "\n",
    "# Store the results in a DataFrame\n",
    "df = spark.createDataFrame(cursor.fetchall(), column_names)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[datetime: timestamp, email: string, name: string, grass_date: date]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n",
    "#type = DataFrame. Equals to df_commit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56762484a8c6ca36c97f8775513e317f6051f1f7831bb533f00135151f0ddf6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
